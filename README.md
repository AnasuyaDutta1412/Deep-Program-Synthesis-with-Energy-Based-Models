# Deep-Program-Synthesis-with-Energy-Based-Models

Current self-supervised learning systems face significant challenges with abstract reasoning. While recently emerging paradigms of inference-time optimisation have led to considerable improvements, they still fundamentally lack the notion of considering Reasoning as being a kind of State Space Search with discrete actions. This research proposes novel architectures for performing abstract reasoning: **Latent Program VAE (LP-VAE)** and **Latent Program DINO (LP-DINO)**. These combine the abstraction power of Deep Learning with the reasoning apparatus of Program Synthesis by learning latent representations of programs. The architectures are evaluated on reasoning datasets (PairedMNIST, ARC-AGI) and compared with a traditional Deep Learning architecture (Vanilla VAE). The results demonstrate that these novel approaches significantly outperform traditional Deep Learning in abstract reasoning tasks.

## Motivation & Problem Statement

Despite advances in AI (e.g., SSL, LLMs), current systems fundamentally struggle with abstract reasoning and procedural understanding, particularly on benchmarks like the Abstract Reasoning Corpus (ARC). This limits generalization and adaptation.

Why the struggle? Standard Deep Learning often suffers from:
1.  **Static Representations:** Ignoring context and the "Frame Problem". (Like knowing a tool but not how its use changes with the job).
2.  **System 1 Dominance:** Relying on fast, reactive pattern matching, failing at deliberate, multi-step reasoning (System 2).
3.  **Lack of Procedural View:** Difficulty generating sequences of steps (programs) without combinatorial explosion. (Like having Lego bricks but no instructions).

This project addresses these gaps by reframing reasoning as a form of planning with internal, procedural steps (programs), drawing parallels with the planning needs emerging in Physical AI.

## Proposed Solution: Latent Program Synthesis with EBMs

Instead of searching the intractable space of discrete programs, we propose **Latent Program Synthesis**:
* Learn a continuous **latent space** where each point represents an entire program or procedure.
* Use the abstraction power of Deep Learning to create this high-level "map".
* Optimize or search within this simpler latent space to find solutions.

**Energy-Based Models (EBMs)** provide the mechanism:
* EBMs assign a low "energy" score to compatible or likely configurations.
* We define an energy function `E(Task, Program)` scoring how well a program fits a task.
* **Inference:** Find the program with the *lowest energy* for a given task ("best tool for the job").

## Models

We developed two novel architectures based on different EBM paradigms:

1.  **Latent Program VAE (LP-VAE):**
    * Based on **Reconstruction** (VAE).
    * Learns by encoding a task `(Input X, Output Y)` into a latent program `Z`, then decoding/executing `Z` on a *new* input `X'` to reconstruct the corresponding `Y'`.
    * Forces `Z` to represent the general procedure, not just memorize the output.

2.  **Latent Program DINO (LP-DINO):**
    * Based on **Contrastive Learning / Self-Distillation** (DINO).
    * Learns by ensuring consistency between latent programs `Z` generated by a 'student' and a stable 'teacher' network for similar task inputs.
    * Reconstruction-free, focuses on distilling the "essence" of the program.
    * Uses techniques (EMA Teacher, Asymmetry, Temperature Scaling) to prevent representation collapse.

## Datasets

Models were evaluated on:

1.  **PairedMNIST:** A novel dataset created for this research to test basic procedural logic (+/- 1) on MNIST digits. Contains pairs `(X, X+1)` or `(X, X-1)`.
2.  **ARC-AGI (subset):** The challenging Abstract Reasoning Corpus benchmark. Used the RE-ARC tool to generate a large dataset (400k samples across 3 tasks) to overcome few-shot limitations for training deep models. Custom data loading was implemented for efficiency (JSONL conversion, PyTorch Dataset class).

## Key Results

* **Accuracy:** Both LP-VAE and LP-DINO significantly outperformed the baseline Vanilla VAE on the challenging ARC-AGI dataset (peak accuracy >80-90% vs. <20% for baseline within 50 epochs). This validates the benefit of learning program representations.
* **Latent Space Quality:**
    * Both models learned qualitatively more structured latent program spaces than the baseline (which showed an unstructured cloud).
    * Quantitative metrics (CH Index, WCSS, Lipschitz) revealed trade-offs: LP-VAE showed better cluster separation and smoothness; LP-DINO showed better compactness.
* **Training Dynamics:** Both models exhibited high peak performance but also showed training volatility, indicating challenges in achieving stable convergence.

## Installation & Setup

```bash
# Placeholder: Add instructions on how to set up the environment
# e.g., git clone ...
# cd deep-program-synthesis-ebm
# conda env create -f environment.yml
# pip install -r requirements.txt
